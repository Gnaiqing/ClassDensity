能不能够把自己的权限提高一点，就可以做任何事情了，如果能取得管理员权限，那就更好了。如果体会不到，那也反正贼不走空嘛，贼既然进了人家的房间，就不会空手离开了，你就看看能捞什么就捞什么，等等。这是第一个要摆脱的，你们以后一定要注意，不要再写那种开着开着，然后数组一千那样或者一万那样的东西了。另外一个就是在哪分配了内存一定要在哪释放内存，千万不要在循环里头、一个函数里头分配内存忘了释放。这两点是我看你们的代码的时候，远远不能够跟你们的理论知识匹配的一个，你们在这方面是光鲜照人的，理论知识和演算、数学推理都很好，可是另外一面就有点寒酸。以后你们就避免，这个刚开始都是这样的，刚开始一定会碰到那种很蠢的事情，看有没有人给你们说出来、告诉你们，你们赶紧把它改好。即使是到我这个年纪了，三四十岁、四五十岁的人了，不管是写代码还是写证明，还是有这种低级的错误，但是一定要有人告诉我，不要丢脸丢得太大，先看看小范围地征集一下，赶紧改，这样才能把自己提高。除了我们提高理论知识以外，还要提高我们的系统的理解和代码的欣赏。也可以把它当做是一些文字的东西，你可以欣赏，就是你们自己也可以知道不要犯一些错误！好，那么，内容就是这个东西，这个点正好是你们期中考试考完、期末考试还远远都没到，然后正好是有一个小小的空窗期是吧？我们下面的内容是属于提高部分，就是下面的课。本科课就是一般也不要提高得太高，我们下面的这三分之一的内容，很多章节了，就是有这么多章！那么这些内容就属于提高部分，可以说是不用考试的。这几章内容不用考试，就你们可以安全地跳过，大约有好几百页，三分之一吧。但是不是说它不重要，它跟信息检索本身没有非常直接的关系，但是它是属于文本处理的一个关键内容，它们最大的特点就是都是用信息检索那一套数据结构的方法在做，就是索引、分类聚类、文档的相似度、距离这些东西，相当于是我们信息检索往外推的一部分。但是如果你们以后去做研究、做工作的时候，这些方法是必须知道的知识——分类和聚类，所以这么早我们也不想太浪费很多的时间，但是又不能不讲。很可能有的同学就是因为一听说不考了，他就根本就不会再看这一部分了，是吧？所以我还是给大家翻一下，只要把这些内容简单地挑重点，挑一点有趣的东西讲一讲，如果你们感兴趣，就照着有兴趣的那一部分，就看那几页，然后也许就有点印象。我们人就像照相一样的，我们实际上是会在我们的潜意识中把它照进去的，只是到了晚上做梦的时候又把它洗干净了，就是从潜意识把它去掉，我们会忘掉百分之多少多少。我就听心理学家说，其实我们人跟小孩一样，跟疯子一样，全是看进去了，它全部照进去，但是它是存在我们自己意识不到的那个地方，到了晚上的时候，它就会把那些内容清空，这样就为下一次留出空间来，如果不清空的话，我们人就会疯掉，跟那些疯子们一样，因为他那个内存不再清洗了，有这么一个问题。好，那我们先这么过一遍，过完以后，然后大体是什么内容，如果很难，你就知道它的难度，如果有趣，就知道以后在哪去找到这些有趣的的地方。我们从分类开始讲，有一半是讲分类，一半是讲聚类，还有一小点是讲降维，向量空间的降维。好，这我们已经不陌生了，这个词是吧？朴素贝叶斯，这个是个德语，为什么英语里头一定要把这个德语留着，为什么不直接去搞一个像样的英语，搞得要不然打字不好打，而且发音也不像，我就奇怪英语一直不创造一个跟这一样的词，可能是因为这个词翻译得太怪了。这指的那个现象，以至于他们不好意思再造一个那样的词了。我们是怎么引入贝叶斯定义的？就是这么引进来的：如果有相关的和不相关的，那么我们就看这个词，同时在相关里头有多少次，在不相关里头有多少次，然后再来看，它到底对我们具体的某一个查询和一个文档有什么贡献。然后我们就引出了先验定义，这是我们的目标函数，但是我们一般不知道，这时候我们就会用一些先验概率分布去展开，屡试不爽，这就是贝叶斯定义。那么很多地方就可以用到这个做法，凡是用了这个做法的：从后验概率不好找，我们用贝叶斯定律换一下，那你这套方法就叫做贝叶斯方法。太多了，贝叶斯估计，我们最大似然估计是一种方法，其实还有贝叶斯的那种估计。好多好多种，凡是用了这个方法，我们就叫做贝叶斯方法，所以它是一个很大的流派。然后，曾经在数学的那帮人里头还发生长达好像是半个世纪的争论，就是贝叶斯统计这个方法到底是不是有效还是什么的，就是好不好，这是个非常有趣的一场世纪大争论，可见这个是多么的重要。数学家那些人，我对这些数学不太懂，就是人家用贝叶斯方法让毛泽东同志理解了统计方法的重要性，就是在农村、在稻田里头去估算粮食产量，你分了多少个颗粒，每个包谷长了多少个栗子，然后等等，根据这个知识就能预言年度粮食产量，这下把他们给惊到了，哎呀这么好，所以他们才开始重视数学，本来人家是不重视这个的。贝叶斯估计在这方面是特别厉害的，你弄些很小的样本，它属于小样本的方法，就是你后面这个条件概率是可以用很小的样本去做的。做完以后，取些例子出来，然后只要你空间选得好，展开空间选得好，就是全概率空间选得好，那么这些条件概率就稳定性很高，这样的话它算出来就会很准。好。现在我们有这个向量空间，我们有了一个索引，可以建立向量空间，也可以作为它的概率模型是吧？那么我们可以用到的地方，首先是分类和聚类，你们学什么自然辩证法之类的这些东西，恩格斯的一句话，一切科学的起始点就是分类。就把你的知识分类，把你的体系分出来，我们把我们的领域分成一层一层的各种类。分类实际上包含两方面的意思，一个是你找到你的那些类、那些标签，并且建成逻辑体系，然后再把这些标签贴到各个样本上面，就贴到各个文档上面。所以分类实际上分两部分，一个标签有了，你把它贴上去，这叫归类。还有一种连标签都没有，只是有样本，那你就得想办法把这些样本怎么区分开来，然后给它们找到标签，然后再贴上。因为分类本质上是广义的分类，就是这么两个体系。但实际上我们分类讲得太多了，我们都是假设大家已经有了标签体系，所以我们现在讲的分类大部分都是狭义的，就是指归类。比如说最简单的两个类01，这种分类太多了。垃圾过滤是吧？一般垃圾就是垃圾，很少有它是百分之一的垃圾和百分之九十的非垃圾，不会这样的。这就是二级的，要么就是垃圾，要么就是非垃圾。所以我们讲的是归类，他的问题描述就是有一个不同种类的集合，种类1到种类n，然后你用一个函数，一般是单值函数，函数都是单值的，但是也有多值的——多标签分类，我们一般都是指单标签分类，单标签就是只贴一个。贝叶斯分类器是一个多标签分类器，它可以多标签，这么一个函数叫做class by分类器，很简单，给定一个函数就是给定一个分类器。比如说文档分类，文档分类就是我们去图书馆，或者是书店，它一个个的书架，就是这样的一个文档分类体系。要一本书、一本期刊，你先找到你的类，首先你找到你的大房间，那就是个大类。文科的、理科的、不文不理的、综合的这样，那么这个属于文档分类，是每天都在做的事，我们的意识都在做，这个很适合自动去分类，我们前面讲的都是人工的，这种分类有很多种了，各种各样的。比如我们每次看新闻的时候不就有频道吗？那些就是分类。怎么样去快速人工分类——菜单，曾经在互联网上也是有过的——开放式分类目录搜索系统ODP，还有很多以前的那些综合门户也是——豆瓣的分类，还有医学里面的，所以现在各个门户还都是分了类的，每个不同的平台，比如说一个专门写狗仔新闻的人和写严肃的药品什么的，他们还是两类完全不同的工作是吧？所以还是得分类。但是我们要讲的是自动分类，就是要用算法去分类，现在都是算法打败人工，就是今日头条那样的东西。今日头条它的每一个频道，都不是人工区分出来的，都是他自动给你分的，而且他不仅把文档分类，还能把你的兴趣爱好也分了类，它会自动去个性化定制每个类。所以这些必须是自动的，人工是不可能做成这样的。就是说以前的网站都是千人一面，就一千个人来看看到都是完全一样的，现在我们的手机上的客户端都是千人千面，是吧？你把你的今日头条的东西拿出来跟你的同学比一下就知道，排在前面的完全是不一样的，那就是因为它个性化你的需求，所以自动的是真正的大数据的方法。所以这些必须是自动的，人工是不可能做成这样。有监督的学习，我们这个方法目前还是最重要的。你每次点击是不是把它翻到底了，它都在记录你的行为。一篇文章从头看到尾了，你那个屏幕它就会一直在往下翻，它就会跟踪你到底看到哪一行了。有个很简单的脚本就可以跟踪你翻到那行，然后他就知道了，无时无刻不在分析你的心。这些东西都是监督。你再指导他，你再给反馈。然后没有指导的他自己去挖掘，那么一般都是混合在用的，这也是方法。特别最主要就是要把先验概率转换一下，找到生成模型怎么去有效的估算后验概率，用这种先验概率。把后验概率换过来，这个就叫做假设假设检验。那么我们就去遍历所有可能的假设，找到最让目标函数最大的假设，我们这个已经不止一次的看到了是吧？好，这个是贝叶斯方法。这么一条就是万变不离其宗，这就是一个学派，其实就这么一个统计学派。贝叶斯估计，就是去变动所有的可能的假设，也就是所有可能的函数这个值。最后整个所谓的朴素贝叶斯分类是什么，跟我们前面讲的二元独立模型，那个方法一样，把每一个词每个特征，都是完全独立的，这样的话就可以把我们去展开。这就是一个朴素贝叶斯分类器，一个文档所有的特征是独立的，所以我们遍历所有的类去找到是最大值的那个类，就去展开。首先每一个类的概率很好算，就是类里面有多少个文档，然后除以总文档，这也是一个类的概率。然后在某个类里头的每个文档是多大的概率？这个你就看词在这个类别里头有多大的概率，然后就把他们乘起来。这就是朴素的，因为这是非常简单的一个假设。但问题是我们会看到它是有用的。比如说这么一个感冒这么一个类别。你看这个报道是不是讲感冒了？可是这个病人在说，他说了一大堆是不是感冒？你看它有哪些特征词，发烧、肉疼，肺肿等等，这就想办法要算一个算一个概率出来。那就有可能是他感冒了。好，所以关键就是要把这些参数给训练出来。所以要平滑是吧？所以这个方法，所以一个方法好不好，就看他的平滑能力怎么样。那么，平滑的问题就是会有零概率，你看你怎么平滑？这就需要专家知识，在这个领域里头需要采取这些领域知识来想办法规划，到底是加一好还是加N好，还是各种各样的方法，这都是很多人在做论证的东西。语言学模型，我们曾经讲过了，我课堂前面的内容，这种方法就是一个很简单是最常用的一个可以是连乘起来，这里的说不能够有一个为0，为0的话就完蛋了是吧？所以在每一个都在想办法估算。在概率论里头不应该有0，最多就是讲极小。为零就是指你的样本还没有碰到，就是说他不可能概率论里面不可能是有哪一个事件是绝对为0的，它一定是很长的尾巴，只是被你截断而已。你的实验次数不太够。好，这是它的学习估算是吧？问题就是碰到时候你得要你在估算的时候就要考虑到你的平滑的东西，而不是在碰到0的时候才进去。每一个非0的东西，你也考虑到他已经在平滑，所以它是一个非常简单的分类器。就任何分类器，你可以认为是你把那些参数估算出来，这个叫做训练。模型训练，然后是开始进行去算出在线的算出来，所以他是一个非常快的方法。如果有些因为浮点数可能溢出，这时候用加法，他就不溢出了，是吧？就32比特的浮点数就足够好了，就不需要用双精度数。这是为什么？我们现在的所有文本处理浮点数就够了，不要用双精度浮点数。好，这是估算。特征选取我们已经前面讲过了，我们想办法把那些什么停用词给去掉，这就是一个很好的自动化的特征选取。而且带平滑的是吧？那么它这里有一些采用的方法，开方统计就是去学，学哪一个特征好呢？我一些假设我不知道哪个特征好，我就回头去猜，算这些东西。这个也可以不要多讲。互信息作为特征选取形，这两个特征好不好？这两个特征相关的就是关联度太高了，我们就只选一个，或者是把他们组合起来。互信息这个东西特别适合做短语识别、新词识别。比如说你的词典里头假设没有收鸳鸯这个词，那么鸳和鸯在我们汉语和基本上都是在一块出现的。但在古代他们当时是分开的了是吧？就是凤是雄性的，所以男的可以叫凤是吧？女的就叫凰，但是为什么现在如果一个有个男生取什么凤，大家就觉得这个人好像没有阳刚之气，是吧？但是这就是因为我们已经把这个词转换了，所以鸳鸯也是这样的，这本来是雄和雌，结果没想到，我们都把它统一用了，只怕家家雄鸳鸯雌鸳鸯。这个时候如果这两个字，这个意思就是说他们这两个词要么就不出现，要出现的话，他们就一定是在一起出现。你去统计一个文档，要么文章里没这个词，一旦这个文章就有这个词，它们俩总是在一块，总是在一个文档里出现，这个就特别适合去找一些新词。比如刚才讲的鸳鸯、玻璃，这些词就是什么？假设你的收入就很容易就找出来。都是一些我们就可以快速过去了，我们量很大。我们主要是看它分类，分类的效果好不好。那么现在的商用产品，就是说你想做一个产品，可能有人想想要一套垃圾邮件过滤系统或者垃圾短信过滤系统，你就去给人家装一套。其中一定要有一个分类器是这个贝叶斯分类器，因为它是速度最快，参数最可控，未必是最好的。总的还是不错的。尤其是有些过滤器基本上都会用到它。这么说，他是挺好用的一个方法，在很多场合里头他表现的相当的稳定，尤其是它的内存什么计算量都要求得非常低，所以它肯定是一个很好的基准了。好，这是他分析，但是我们一会会跟别的比一比，你会发现它实际上有时候太简单了，把一些特征丢掉。他只是然后如果文本量很大的话，它未必可行。下面我们讲一个高级一点的分类器，向量空间怎么分类？朴素贝叶斯是没有对文本做任何假设，是吧？现在我们把我们假设成一个向量，这个时候我们就会有两种方法。一种就是根据他们的距离之间来算，一个是根据他们的能不能聚类，根据夹角那些东西来算。向量空间我们都比较熟悉了，这是主要是权重。而且很直观，他就特别适合这种划分文档样本空间，关键是你怎么去划分它呢？假设你已经划分好了，来了一个新文档，你就想办法给他们找出边界，这个边界就是根据距离信息来得得到的，所以这是很容易分离的方法。KNN这是一个很好的分类方法，甚至统计学里头证明K总是可以得到最好的结果，可以理论上得到最优的结果，你只要改把NK变大一点。K是什么呢？就是一个模型参数，你选一个你的计算量最容易的，一般是3~5就已经很好了。极少有超过8的。一般三个或者5个。比如说6个了一个比较好的类型，他就看他6个最近的朋友，我们想判断一个人，比如说你去银行借钱，借钱买房子。一般他就去看你的信用是吧？看你是不是干嘛坏事。但是光靠这个还不行，他还要看你的亲戚朋友，看6个人。6度分割是吧？我看你的6个朋友或者是你的同学，一般他会要求你讲很多人，然后他就会做一个挑几个，然后他就开始看那些人是什么情况，给他们打电话问那些人对你是什么评价？好，这个时候它就基本上就有谱了，如果你这么下来还没谱，说明他你这个人实在是太厉害了，让人难以琢磨，无法分类。那么，一般看6个最近的邻居，用向量空间就可以算出最近了。然后看看每个人投一票，然后就找最多的那一票，那一盘就把你划到你跟他们是一路的。你的朋友是在这一步，我就把他画到这里去。所以想法也是很简单。问题就是怎么去训练出来，要把文档给表现出来。这个时候一般都是要把文档向量空间建好，然后要得到训练集，这训练集是人工已经给你标好，可能专家也校对过了。这种方法，实际上就是一个记忆法是吧？我把所有的资料都都记好了，来了一个新的东西的时候，我就去跟以前比去，因为太多了我就比6个。我相信福尔摩斯探案集，他的很多案例好像都是用这种方法。所以这种方法也叫做看看案例。然后每来一个新的案例，他就去跟这些找几个最近的，所以他这个方法是很有效的。虽然他学的时候很用功，但是他分类的时候是很懒的。KNN这个方法本身它比贝叶斯方法好的地方，就是在数学上面，根据各种各样的损失函数来说，他的概率也就是风险比贝叶斯要小，这是数学上可以证明的。他最大的问题是怎么样算距离，这个有点关键，不同的距离度量会得出不同的结果。这里一个案例就是3近邻，用夹角也可以算算这三个的距离。那么总结一下，就是说无论倒排索引来做这件事情，我们选B这么多个文档，然后要对所有的词，因为要算距离，然后这个文档的长度一般不会选很多词了。很多文档，那就去训练它，算距离。这就是一个KNN决策。那么他跟贝叶斯正好是两个极端。KNN是相当于死记硬背型的人才，什么都记下来，死记硬背。好处当然是他分类只要很近，只要是在这个领域里头他就是最优的。坏处就是它适应能力会很差，一旦换了一套题型，换了一套问题，你们在中学都老碰到这种同学是吧？有的东西就是用功的记下来，题海战术，但是如果换一个新的题型，它可能因为没见过的，那他就不知道要跟谁去比了。跟所有的邻居都很远，就是这个意思。这就是个高方差低偏差。朴素贝叶斯反过来，他学的东西很少，每个文章只抽取那么一点点特征。但是它的好处就是它适应多种题型，各种样本变化，也就他很多东西他没有记下来，她不知道。所以你来骗我也骗不着我，我根本没看见你这些那些线索。就这样，所以它是有稳定性。这是两种，这是一个死记硬背型的极端例子。像KNN它是各种各样的可能性都是能划分出来的，因为毕竟找最近的，但是我们就是一个线性分类模型，所以碰到这个事情他就没办法。但是KNN是有办法，贝斯叫他要用一个平板把它分开。还有一种方法就是改进这种二次分类，就用多元的分类。我们下面讲另外一种分类方法，是想办法找到他们介于他们之间的一个。这个方法是一种叫做感知器。他的目的很简单，你有一套样本，给你他样本，你先训练样本，训练出一个函数出来，让他们之间的差别，他们两个之间的距离最大化。想办法让他们空间中间最大化，然后把他们都隔开，这样来做分类。这样的话有可能能找到一个新的稳定的一点，不需要记那么多，只需要学一点点。我们下一个PPT要讲的，就是SVM。我们只是说线性分类器作为一个对有这么多可能性。它是线性排列，你只要取个对数，他就变成一个平面，它就是一个线性分类器。高维空间的情况下，也就是只要是各个文档是可分的，她还是做得很好，是像这种如果是难以分开，混合在一块，显然是KNN能做得更好。多个类别那就要做两两分类，如果有这样这三种，那你就做成三个。这个就找最近的那个。所以这个贝叶斯分类是做一个线性分类器，就是在这方面就需要有些技巧。另外一个就是用向量空间模型，不用那些概率，那么我们Rocchio定理今天我们讲过了。指的是在同样的查询下面会找一个最优的一个城市，是离相关文档最近，但是离不相关文档最远，同时处理这两种情况，那么他怎么去分类呢？它这个分类关键是要对负相关文章，负面的一些案例要要一个很好的度量，就那些参数要学好。决策树，也是一个分类，1层1层的去算相似性，最后算到最有可能的那种。这些我们可以翻过去，我们讲下一个分类器，SVM。那么下面这个方法是想办法在学习方面找到一种折中，既不像朴素贝叶斯那样只学那么一点点特征，也不像KNN那样把所有的都记下来，而是找一种在界面上的那些特征，要让他们的距离最大的他们之间隔的那两个平面，他这个方案就是说我们只需要这些文档，只学记住这些文章，这些文就不要了。把这些文章找出来叫做支撑向量。为什么叫SVM呢，是因为它最早用在数学的一些模型上面，所以那些模型都是都把它叫做机器，实际上就是一些函数，这一个分类器就是函数，我们把它下面就把这些学出来。那么这个目前是效果最好的、性能最好的分类器。看来这么讲下去时间不太够，我下面就会非常快的讲，我先把支撑向量机讲完，然后我们再歇一会。因为他还是有点重要，你们以后肯定会碰到分类器，虽然我们现在不考。总之你知道他选样本的时候，他是学这样的，好吧。那么怎么去学呢？如果有些项目不小心混在里头，他也有些层次、方法。好，总之这个这么快的将过去。他就是一个简单的这样的函数把它算出来，去挑这些部分，把这个挑出来，最后就一堆参数，这是一个带拉格朗日乘子的。首先我们要找到这个函数，找到分类器，那么条件是什么呢？肯定是让他们之间的法向量的距离最小。那么，这个就是一个拉格朗日乘子的方法，最后就把这些函数算出来，算出来是什么样的呢？这个W参数是这样的，如果不小心混了，红的里头混了蓝的，蓝的里头混了红的呢？这个时候你就又加一下拉格朗日乘子，把它混在里面叫做松弛因子，叫做松弛变量，就是一些很懒散的变量，那么又变成一个拉格朗日乘子的方法，你只要是条件，又是优化，就可以把这些松弛函数也给算出来。好，总之就这样。一旦算出来以后，它分类就很简单了是吧，你就要算这个函数，这个分类器，它如果大于这个距离就在上面，小于这个距离在下面，如果在中间就不确定。所以只要是不落在他的中间的这些分类都非常的精确，所以他的学习过程很简单，而且他它还可以扩展到非线性的情况，像这些什么极坐标、各种坐标，像极坐标系统，做一个变换就变成了平面坐标系统是吧，这种变化方法叫做核函数，是个把戏，有各种各样的核函数，不一定非要是这种。在前面加一个字母，把每个坐标变成另外一个坐标，然后在前面乘一个核函数，比如说高斯分布的一种核函数，XY两个坐标最后变成了这样。那么评测结果是什么呢？他们参加文本分类评测，有路透社的来分类，路透社的东西都是分得很好的分类，我们看结果。也就是说这个分类器是最好的，这是最普通的向量分类器百分之六十四，朴素贝叶斯也不错，然后决策树提高了不少。但是跟这个相比还是这个是最好的。SVM这个问题现在也很成熟，你可以在网上很容易找到一个叫SVM库函数的一个包。还有现在很多人在更新。如果你要碰到分类器，下载SVM就可以做，就会发现它是一个最好的分类器，目前性能最好。我们先休息一下，下节课我们再赶进度。是什么数据量的训练集，数据量多不多。把这个作业给大家发一下。因为我们这节课还要讲一下报告的问题。我们这时候定下来。这些情况应该怎么测，还有就是你这个类别应该怎么测，类和类之间是否清楚，模棱两可就不清楚。数据量小就不能似然估计，这个时候小样本估计的贝叶斯估计的方法就很好。关键是要学好条件概率，其他的方法就是死记硬背的方法不行，数据量不错，应该第一个考虑SVM，SVM的复杂性还是比朴素贝叶斯要高很多，很慢。所以用起来还是要小心。但是一旦找到支撑向量，后面又很快了，大量的数据怎么办呢，就要什么都要试一下，不要嫌麻烦，一般写论文，都假设找到了足够的数据，不然不够格，功夫没下够，是最常见的情况。你的结论是，把各种方法都试一遍，并且找到又快又好的方法，比如最近邻、朴素贝叶斯、SVM等等各种方法，你都得试一试。
